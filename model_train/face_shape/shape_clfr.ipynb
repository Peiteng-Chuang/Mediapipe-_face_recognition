{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers, regularizers,models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import cv2  \n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "path_0 = './0_round'\n",
    "path_1 = './1_square'\n",
    "path_2 = './2_heart'\n",
    "path_3 = './3_long'\n",
    "path_4 = './4_oval'\n",
    "path_5 = './5_triangle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(241, 150, 150, 1)\n",
      "(241, 6)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5]\n"
     ]
    }
   ],
   "source": [
    "features, labels = [], []\n",
    "#===========================================讀取第0項分類\n",
    "for file_name in os.listdir(path_0):\n",
    "    if file_name.endswith('.jpg'):\n",
    "        img_path = os.path.join(path_0, file_name)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (150, 150))\n",
    "            features.append(img)\n",
    "            labels.append(0)  # Use 0 for round face\n",
    "            img = img/255.0\n",
    "\n",
    "#===========================================讀取第1項分類\n",
    "for file_name in os.listdir(path_1):\n",
    "    if file_name.endswith('.jpg'):\n",
    "        img_path = os.path.join(path_1, file_name)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (150, 150))\n",
    "            features.append(img)\n",
    "            labels.append(1)  # Use 1 for square face\n",
    "            img = img/255.0\n",
    "\n",
    "#===========================================讀取第2項分類\n",
    "for file_name in os.listdir(path_2):\n",
    "    if file_name.endswith('.jpg'):\n",
    "        img_path = os.path.join(path_2, file_name)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (150, 150))\n",
    "            features.append(img)\n",
    "            labels.append(2)  # Use 2 for heart face\n",
    "            img = img/255.0\n",
    "\n",
    "#===========================================讀取第3項分類\n",
    "for file_name in os.listdir(path_3):\n",
    "    if file_name.endswith('.jpg'):\n",
    "        img_path = os.path.join(path_3, file_name)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (150, 150))\n",
    "            features.append(img)\n",
    "            labels.append(3)  # Use 3 for long face\n",
    "            img = img/255.0\n",
    "\n",
    "#===========================================讀取第4項分類\n",
    "for file_name in os.listdir(path_4):\n",
    "    if file_name.endswith('.jpg'):\n",
    "        img_path = os.path.join(path_4, file_name)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (150, 150))\n",
    "            features.append(img)\n",
    "            labels.append(4)  # Use 4 for oval face\n",
    "            img = img/255.0\n",
    "\n",
    "#===========================================讀取第5項分類\n",
    "for file_name in os.listdir(path_5):\n",
    "    if file_name.endswith('.jpg'):\n",
    "        img_path = os.path.join(path_5, file_name)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (150, 150))\n",
    "            features.append(img)\n",
    "            labels.append(5)  # Use 5 for triangle face\n",
    "            img = img/255.0\n",
    "\n",
    "#======================================================\n",
    "# Convert lists to numpy arrays and normalize the images\n",
    "features = np.asarray(features, np.float32) / 255.0\n",
    "labels = np.asarray(labels, np.int32)\n",
    "\n",
    "# Reshape images to match the input shape of the neural network 4d\n",
    "train_images = features.reshape((-1, 150, 150, 1))\n",
    "train_labels = labels\n",
    "\n",
    "#===========================train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(train_images, train_labels, test_size=0.2, random_state=6)\n",
    "\n",
    "# y label One-hot encode\n",
    "y_train = to_categorical(y_train, num_classes=6)\n",
    "y_val = to_categorical(y_val, num_classes=6)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make an ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "batch_size = 32\n",
    "train_generator = datagen.flow(x_train, y_train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76832</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,917,312</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">390</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76832\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │     \u001b[38;5;34m4,917,312\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │           \u001b[38;5;34m390\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,918,022</span> (18.76 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,918,022\u001b[0m (18.76 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,918,022</span> (18.76 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,918,022\u001b[0m (18.76 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## build CNN model\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 1)))\n",
    "model.add(layers.MaxPooling2D((3, 3)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(6, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: [1.07092199 1.0942029  0.9869281  1.29059829 0.52982456 2.09722222]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# 假设你有一个类别标签数组 train_labels，例如 [0, 1, 1, 2, 0, 1, ...]\n",
    "\n",
    "# 计算类别权重\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "\n",
    "# 打印类别权重\n",
    "print(\"Class Weights:\", class_weights)\n",
    "\n",
    "class_weights_dict = dict(enumerate(class_weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 71ms/step - accuracy: 0.1660 - loss: 4.0311 - val_accuracy: 0.0000e+00 - val_loss: 2.2632\n",
      "Epoch 2/20\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - accuracy: 0.2036 - loss: 1.7276 - val_accuracy: 0.0000e+00 - val_loss: 2.3993\n",
      "Epoch 3/20\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - accuracy: 0.1798 - loss: 1.6983 - val_accuracy: 0.0000e+00 - val_loss: 2.6612\n",
      "Epoch 4/20\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - accuracy: 0.1900 - loss: 1.7146 - val_accuracy: 0.0000e+00 - val_loss: 2.6082\n",
      "Epoch 5/20\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - accuracy: 0.2134 - loss: 1.6706 - val_accuracy: 0.0000e+00 - val_loss: 2.9693\n",
      "Epoch 6/20\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - accuracy: 0.2097 - loss: 1.6207 - val_accuracy: 0.0000e+00 - val_loss: 3.0665\n",
      "Epoch 7/20\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - accuracy: 0.2329 - loss: 1.6458 - val_accuracy: 0.0000e+00 - val_loss: 2.8857\n",
      "Epoch 8/20\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - accuracy: 0.2312 - loss: 1.5820 - val_accuracy: 0.0000e+00 - val_loss: 3.2212\n",
      "Epoch 9/20\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - accuracy: 0.2058 - loss: 1.5946 - val_accuracy: 0.0000e+00 - val_loss: 3.0432\n",
      "Epoch 10/20\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - accuracy: 0.2091 - loss: 1.6046 - val_accuracy: 0.0000e+00 - val_loss: 3.0196\n",
      "Epoch 11/20\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - accuracy: 0.1675 - loss: 1.5916 - val_accuracy: 0.0000e+00 - val_loss: 3.2384\n",
      "Epoch 12/20\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - accuracy: 0.1732 - loss: 1.5632 - val_accuracy: 0.0000e+00 - val_loss: 3.5146\n",
      "Epoch 13/20\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - accuracy: 0.1772 - loss: 1.5840 - val_accuracy: 0.0000e+00 - val_loss: 3.7580\n",
      "Epoch 14/20\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - accuracy: 0.1618 - loss: 1.5704 - val_accuracy: 0.0000e+00 - val_loss: 3.3131\n",
      "Epoch 15/20\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 71ms/step - accuracy: 0.1855 - loss: 1.6046 - val_accuracy: 0.0000e+00 - val_loss: 3.2343\n",
      "Epoch 16/20\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - accuracy: 0.1740 - loss: 1.5636 - val_accuracy: 0.0000e+00 - val_loss: 4.2141\n",
      "Epoch 17/20\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - accuracy: 0.1608 - loss: 1.5867 - val_accuracy: 0.0000e+00 - val_loss: 3.0063\n",
      "Epoch 18/20\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - accuracy: 0.1659 - loss: 1.5537 - val_accuracy: 0.0000e+00 - val_loss: 3.7689\n",
      "Epoch 19/20\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - accuracy: 0.1525 - loss: 1.5497 - val_accuracy: 0.0000e+00 - val_loss: 2.9976\n",
      "Epoch 20/20\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 73ms/step - accuracy: 0.1806 - loss: 1.5803 - val_accuracy: 0.0000e+00 - val_loss: 3.6085\n"
     ]
    }
   ],
   "source": [
    "train_history=model.fit(\n",
    "    x=train_images,\n",
    "    y=to_categorical(train_labels),\n",
    "    validation_split=0.2,\n",
    "    epochs=20,\n",
    "    batch_size=5,\n",
    "    class_weight=class_weights_dict\n",
    ")\n",
    "# 使用增強數據生成器進行訓練\n",
    "# model.fit(train_generator,\n",
    "#           steps_per_epoch=len(x_train) // batch_size,\n",
    "#           epochs=20,\n",
    "#           validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#k-fold\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "# 对标签进行One-hot编码\n",
    "train_labels = to_categorical(train_labels, num_classes=6)\n",
    "\n",
    "# 数据增广\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# K折交叉验证\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold = 1\n",
    "val_accuracies = []\n",
    "\n",
    "for train_index, val_index in kf.split(train_images):\n",
    "    x_train, x_val = train_images[train_index], train_images[val_index]\n",
    "    y_train, y_val = train_labels[train_index], train_labels[val_index]\n",
    "\n",
    "    # 创建模型\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(16, (3, 3), activation='relu', input_shape=(150, 150, 1)))\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(6, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # 回调函数\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "    \n",
    "    # 生成训练数据\n",
    "    batch_size = 2\n",
    "    train_generator = datagen.flow(x_train, y_train, batch_size=batch_size)\n",
    "    \n",
    "    # 训练模型\n",
    "    history = model.fit(train_generator,\n",
    "                        steps_per_epoch=len(x_train) // batch_size,\n",
    "                        epochs=20,\n",
    "                        validation_data=(x_val, y_val),\n",
    "                        callbacks=[early_stopping, reduce_lr])\n",
    "    \n",
    "    # 记录验证准确率\n",
    "    val_accuracy = history.history['val_accuracy'][-1]\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    print(f'Fold {fold}, Validation Accuracy: {val_accuracy}')\n",
    "    fold += 1\n",
    "\n",
    "# 输出平均验证准确率\n",
    "print(f'Average Validation Accuracy: {np.mean(val_accuracies)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('./test_data/square.jpg',cv2.IMREAD_GRAYSCALE)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(cv2.cvtColor(img,cv2.IMREAD_GRAYSCALE))\n",
    "img = cv2.resize(img, (150, 150))\n",
    "img = img/255.0\n",
    "img = np.asarray(img).reshape(-1,150, 150,1 )\n",
    "\n",
    "result = np.argmax(model.predict(img))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('./test_data/heart.jpg',cv2.IMREAD_GRAYSCALE)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(cv2.cvtColor(img,cv2.IMREAD_GRAYSCALE))\n",
    "img = cv2.resize(img, (150, 150))\n",
    "img = img/255.0\n",
    "img = np.asarray(img).reshape(-1,150, 150,1 )\n",
    "\n",
    "result = np.argmax(model.predict(img))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('./test_data/long.jpg',cv2.IMREAD_GRAYSCALE)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(cv2.cvtColor(img,cv2.IMREAD_GRAYSCALE))\n",
    "img = cv2.resize(img, (150, 150))\n",
    "img = img/255.0\n",
    "img = np.asarray(img).reshape(-1,150, 150,1 )\n",
    "\n",
    "result = np.argmax(model.predict(img))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('./test_data/round.jpg',cv2.IMREAD_GRAYSCALE)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(cv2.cvtColor(img,cv2.IMREAD_GRAYSCALE))\n",
    "img = cv2.resize(img, (150, 150))\n",
    "img = img/255.0\n",
    "img = np.asarray(img).reshape(-1,150, 150,1 )\n",
    "\n",
    "result = np.argmax(model.predict(img))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('./test_data/trangle.jpg',cv2.IMREAD_GRAYSCALE)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(cv2.cvtColor(img,cv2.IMREAD_GRAYSCALE))\n",
    "img = cv2.resize(img, (150, 150))\n",
    "img = img/255.0\n",
    "img = np.asarray(img).reshape(-1,150, 150,1 )\n",
    "\n",
    "result = np.argmax(model.predict(img))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_save/face_classfier_V0.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_39_cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
