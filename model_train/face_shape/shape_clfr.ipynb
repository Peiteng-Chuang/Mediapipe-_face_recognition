{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers, regularizers,models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import cv2  \n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "path_0 = './0_round'\n",
    "path_1 = './1_square'\n",
    "path_2 = './2_heart'\n",
    "path_3 = './3_long'\n",
    "path_4 = './4_oval'\n",
    "path_5 = './5_triangle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(241, 150, 150, 1)\n",
      "(241, 6)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5]\n"
     ]
    }
   ],
   "source": [
    "features, labels = [], []\n",
    "#===========================================讀取第0項分類\n",
    "for file_name in os.listdir(path_0):\n",
    "    if file_name.endswith('.jpg'):\n",
    "        img_path = os.path.join(path_0, file_name)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (150, 150))\n",
    "            features.append(img)\n",
    "            labels.append(0)  # Use 0 for round face\n",
    "            img = img/255.0\n",
    "\n",
    "#===========================================讀取第1項分類\n",
    "for file_name in os.listdir(path_1):\n",
    "    if file_name.endswith('.jpg'):\n",
    "        img_path = os.path.join(path_1, file_name)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (150, 150))\n",
    "            features.append(img)\n",
    "            labels.append(1)  # Use 1 for square face\n",
    "            img = img/255.0\n",
    "\n",
    "#===========================================讀取第2項分類\n",
    "for file_name in os.listdir(path_2):\n",
    "    if file_name.endswith('.jpg'):\n",
    "        img_path = os.path.join(path_2, file_name)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (150, 150))\n",
    "            features.append(img)\n",
    "            labels.append(2)  # Use 2 for heart face\n",
    "            img = img/255.0\n",
    "\n",
    "#===========================================讀取第3項分類\n",
    "for file_name in os.listdir(path_3):\n",
    "    if file_name.endswith('.jpg'):\n",
    "        img_path = os.path.join(path_3, file_name)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (150, 150))\n",
    "            features.append(img)\n",
    "            labels.append(3)  # Use 3 for long face\n",
    "            img = img/255.0\n",
    "\n",
    "#===========================================讀取第4項分類\n",
    "for file_name in os.listdir(path_4):\n",
    "    if file_name.endswith('.jpg'):\n",
    "        img_path = os.path.join(path_4, file_name)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (150, 150))\n",
    "            features.append(img)\n",
    "            labels.append(4)  # Use 4 for oval face\n",
    "            img = img/255.0\n",
    "\n",
    "#===========================================讀取第5項分類\n",
    "for file_name in os.listdir(path_5):\n",
    "    if file_name.endswith('.jpg'):\n",
    "        img_path = os.path.join(path_5, file_name)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (150, 150))\n",
    "            features.append(img)\n",
    "            labels.append(5)  # Use 5 for triangle face\n",
    "            img = img/255.0\n",
    "\n",
    "#======================================================\n",
    "# Convert lists to numpy arrays and normalize the images\n",
    "features = np.asarray(features, np.float32) / 255.0\n",
    "labels = np.asarray(labels, np.int32)\n",
    "\n",
    "# Reshape images to match the input shape of the neural network 4d\n",
    "train_images = features.reshape((-1, 150, 150, 1))\n",
    "train_labels = labels\n",
    "\n",
    "#===========================train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(train_images, train_labels, test_size=0.2, random_state=6)\n",
    "\n",
    "# y label One-hot encode\n",
    "y_train = to_categorical(y_train, num_classes=6)\n",
    "y_val = to_categorical(y_val, num_classes=6)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make an ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "batch_size = 32\n",
    "train_generator = datagen.flow(x_train, y_train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76832</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,917,312</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">390</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76832\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │     \u001b[38;5;34m4,917,312\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │           \u001b[38;5;34m390\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,918,022</span> (18.76 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,918,022\u001b[0m (18.76 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,918,022</span> (18.76 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,918,022\u001b[0m (18.76 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## build CNN model\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 1)))\n",
    "model.add(layers.MaxPooling2D((3, 3)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(6, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: [1.07092199 1.0942029  0.9869281  1.29059829 0.52982456 2.09722222]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# 假设你有一个类别标签数组 train_labels，例如 [0, 1, 1, 2, 0, 1, ...]\n",
    "\n",
    "# 计算类别权重\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "\n",
    "# 打印类别权重\n",
    "print(\"Class Weights:\", class_weights)\n",
    "\n",
    "class_weights_dict = dict(enumerate(class_weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 71ms/step - accuracy: 0.1660 - loss: 4.0311 - val_accuracy: 0.0000e+00 - val_loss: 2.2632\n",
      "Epoch 2/20\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - accuracy: 0.2036 - loss: 1.7276 - val_accuracy: 0.0000e+00 - val_loss: 2.3993\n",
      "Epoch 3/20\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - accuracy: 0.1798 - loss: 1.6983 - val_accuracy: 0.0000e+00 - val_loss: 2.6612\n",
      "Epoch 4/20\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - accuracy: 0.1900 - loss: 1.7146 - val_accuracy: 0.0000e+00 - val_loss: 2.6082\n",
      "Epoch 5/20\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - accuracy: 0.2134 - loss: 1.6706 - val_accuracy: 0.0000e+00 - val_loss: 2.9693\n",
      "Epoch 6/20\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - accuracy: 0.2097 - loss: 1.6207 - val_accuracy: 0.0000e+00 - val_loss: 3.0665\n",
      "Epoch 7/20\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - accuracy: 0.2329 - loss: 1.6458 - val_accuracy: 0.0000e+00 - val_loss: 2.8857\n",
      "Epoch 8/20\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - accuracy: 0.2312 - loss: 1.5820 - val_accuracy: 0.0000e+00 - val_loss: 3.2212\n",
      "Epoch 9/20\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - accuracy: 0.2058 - loss: 1.5946 - val_accuracy: 0.0000e+00 - val_loss: 3.0432\n",
      "Epoch 10/20\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - accuracy: 0.2091 - loss: 1.6046 - val_accuracy: 0.0000e+00 - val_loss: 3.0196\n",
      "Epoch 11/20\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - accuracy: 0.1675 - loss: 1.5916 - val_accuracy: 0.0000e+00 - val_loss: 3.2384\n",
      "Epoch 12/20\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 68ms/step - accuracy: 0.1732 - loss: 1.5632 - val_accuracy: 0.0000e+00 - val_loss: 3.5146\n",
      "Epoch 13/20\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 69ms/step - accuracy: 0.1772 - loss: 1.5840 - val_accuracy: 0.0000e+00 - val_loss: 3.7580\n",
      "Epoch 14/20\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - accuracy: 0.1618 - loss: 1.5704 - val_accuracy: 0.0000e+00 - val_loss: 3.3131\n",
      "Epoch 15/20\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 71ms/step - accuracy: 0.1855 - loss: 1.6046 - val_accuracy: 0.0000e+00 - val_loss: 3.2343\n",
      "Epoch 16/20\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - accuracy: 0.1740 - loss: 1.5636 - val_accuracy: 0.0000e+00 - val_loss: 4.2141\n",
      "Epoch 17/20\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - accuracy: 0.1608 - loss: 1.5867 - val_accuracy: 0.0000e+00 - val_loss: 3.0063\n",
      "Epoch 18/20\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - accuracy: 0.1659 - loss: 1.5537 - val_accuracy: 0.0000e+00 - val_loss: 3.7689\n",
      "Epoch 19/20\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - accuracy: 0.1525 - loss: 1.5497 - val_accuracy: 0.0000e+00 - val_loss: 2.9976\n",
      "Epoch 20/20\n",
      "\u001b[1m49/49\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 73ms/step - accuracy: 0.1806 - loss: 1.5803 - val_accuracy: 0.0000e+00 - val_loss: 3.6085\n"
     ]
    }
   ],
   "source": [
    "train_history=model.fit(\n",
    "    x=train_images,\n",
    "    y=to_categorical(train_labels),\n",
    "    validation_split=0.2,\n",
    "    epochs=20,\n",
    "    batch_size=5,\n",
    "    class_weight=class_weights_dict\n",
    ")\n",
    "# 使用增強數據生成器進行訓練\n",
    "# model.fit(train_generator,\n",
    "#           steps_per_epoch=len(x_train) // batch_size,\n",
    "#           epochs=20,\n",
    "#           validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#k-fold\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "# 对标签进行One-hot编码\n",
    "train_labels = to_categorical(train_labels, num_classes=6)\n",
    "\n",
    "# 数据增广\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# K折交叉验证\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold = 1\n",
    "val_accuracies = []\n",
    "\n",
    "for train_index, val_index in kf.split(train_images):\n",
    "    x_train, x_val = train_images[train_index], train_images[val_index]\n",
    "    y_train, y_val = train_labels[train_index], train_labels[val_index]\n",
    "\n",
    "    # 创建模型\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(16, (3, 3), activation='relu', input_shape=(150, 150, 1)))\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(6, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # 回调函数\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "    \n",
    "    # 生成训练数据\n",
    "    batch_size = 2\n",
    "    train_generator = datagen.flow(x_train, y_train, batch_size=batch_size)\n",
    "    \n",
    "    # 训练模型\n",
    "    history = model.fit(train_generator,\n",
    "                        steps_per_epoch=len(x_train) // batch_size,\n",
    "                        epochs=20,\n",
    "                        validation_data=(x_val, y_val),\n",
    "                        callbacks=[early_stopping, reduce_lr])\n",
    "    \n",
    "    # 记录验证准确率\n",
    "    val_accuracy = history.history['val_accuracy'][-1]\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    print(f'Fold {fold}, Validation Accuracy: {val_accuracy}')\n",
    "    fold += 1\n",
    "\n",
    "# 输出平均验证准确率\n",
    "print(f'Average Validation Accuracy: {np.mean(val_accuracies)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#載入舊模型\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "load_model_path='C:/DL_model/best_model.h5'\n",
    "model = load_model(load_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model input shape: (150, 150, 1)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2' has no attribute 'Grayscle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 92\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, image_path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(image_paths):\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(axes):\n\u001b[1;32m---> 92\u001b[0m         \u001b[43mpredict_and_plot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m plt\u001b[38;5;241m.\u001b[39mtight_layout()\n\u001b[0;32m     95\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "Cell \u001b[1;32mIn[15], line 64\u001b[0m, in \u001b[0;36mpredict_and_plot\u001b[1;34m(image_path, model, ax)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # 將 BGR 轉換為 RGB\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(img, \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGrayscle\u001b[49m)  \u001b[38;5;66;03m# 將 BGR 轉換為 RGB\u001b[39;00m\n\u001b[0;32m     65\u001b[0m img_resized \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(img, (input_shape[\u001b[38;5;241m1\u001b[39m], input_shape[\u001b[38;5;241m0\u001b[39m]))  \u001b[38;5;66;03m# 調整大小至模型所需的輸入大小\u001b[39;00m\n\u001b[0;32m     66\u001b[0m img_resized \u001b[38;5;241m=\u001b[39m img_resized \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m  \u001b[38;5;66;03m# 正規化\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'cv2' has no attribute 'Grayscle'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x600 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Assuming 'model' is your trained model\n",
    "\n",
    "# # Function to predict and plot images\n",
    "# def predict_and_plot(image_path, model, ax):\n",
    "#     # Load and preprocess the image\n",
    "#     img = cv2.imread(image_path)\n",
    "#     img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "#     img_resized = cv2.resize(img, (150, 150))\n",
    "#     img_resized = img_resized / 255.0\n",
    "#     img_resized = np.expand_dims(img_resized, axis=0)  # Add batch dimension\n",
    "    \n",
    "#     # Predict using the model\n",
    "#     prediction = model.predict(img_resized)\n",
    "    \n",
    "#     class_label = np.argmax(prediction)\n",
    "#     prob_distribution = prediction.squeeze()\n",
    "#     prob_distribution_str = '\\n'.join([f'Class {i}: {prob:.2f}' for i, prob in enumerate(prob_distribution)])\n",
    "#     # Plot the image with predicted label\n",
    "#     ax.imshow(img)\n",
    "#     ax.set_title(f'Predicted Class: {class_label}')\n",
    "#     ax.axis('off')\n",
    "#     ax.text(10, 20, f'Prob:\\n{prob_distribution_str}', fontsize=20, ha='left', va='top', color='green')\n",
    "\n",
    "\n",
    "\n",
    "# # List of image paths\n",
    "# image_paths = ['./test_data/0.jpg', './test_data/1.jpg', './test_data/2.jpg',\n",
    "#                './test_data/3.jpg', './test_data/4.jpg','./test_data/5.jpg']\n",
    "\n",
    "# # Create subplots for each image\n",
    "# fig, axes = plt.subplots(1, 6, figsize=(20, 6))\n",
    "\n",
    "# # Iterate over each image path and plot\n",
    "# for i, image_path in enumerate(image_paths):\n",
    "#     predict_and_plot(image_path, model, axes[i])\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "\n",
    "# 確保模型已載入\n",
    "model_path = 'C:/DL_model/best_model.h5'\n",
    "model = load_model(model_path)\n",
    "\n",
    "# 確認模型的輸入形狀\n",
    "input_shape = model.input_shape[1:4]  # 排除批次維度，只取形狀 (height, width, channels)\n",
    "print(f\"Model input shape: {input_shape}\")\n",
    "\n",
    "# 定義預測和繪圖函數\n",
    "def predict_and_plot(image_path, model, ax):\n",
    "    # 載入並預處理圖像\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"Error loading image at {image_path}\")\n",
    "        return\n",
    "    \n",
    "    # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # 將 BGR 轉換為 RGB\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # 將 BGR 轉換為 RGB\n",
    "    img_resized = cv2.resize(img, (input_shape[1], input_shape[0]))  # 調整大小至模型所需的輸入大小\n",
    "    img_resized = img_resized / 255.0  # 正規化\n",
    "    img_resized = np.expand_dims(img_resized, axis=0)  # 增加批次維度\n",
    "    \n",
    "    # 使用模型進行預測\n",
    "    prediction = model.predict(img_resized)\n",
    "    \n",
    "    class_label = np.argmax(prediction)  # 獲取預測的類別\n",
    "    prob_distribution = prediction.squeeze()  # 獲取概率分佈\n",
    "    prob_distribution_str = '\\n'.join([f'Class {i}: {prob:.2f}' for i, prob in enumerate(prob_distribution)])\n",
    "    \n",
    "    # 繪製圖像和預測標籤\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f'Predicted Class: {class_label}')\n",
    "    ax.axis('off')\n",
    "    ax.text(10, 20, f'Prob:\\n{prob_distribution_str}', fontsize=12, ha='left', va='top', color='green')\n",
    "\n",
    "# 圖像路徑列表\n",
    "image_paths = ['./test_data/0.jpg', './test_data/1.jpg', './test_data/2.jpg',\n",
    "               './test_data/3.jpg', './test_data/4.jpg','./test_data/5.jpg']\n",
    "\n",
    "# 為每個圖像創建子圖\n",
    "fig, axes = plt.subplots(1, 6, figsize=(20, 6))\n",
    "\n",
    "# 遍歷每個圖像路徑並繪製\n",
    "for i, image_path in enumerate(image_paths):\n",
    "    if i < len(axes):\n",
    "        predict_and_plot(image_path, model, axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('./test_data/square.jpg',cv2.IMREAD_GRAYSCALE)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(cv2.cvtColor(img,cv2.IMREAD_GRAYSCALE))\n",
    "img = cv2.resize(img, (150, 150))\n",
    "img = img/255.0\n",
    "img = np.asarray(img).reshape(-1,150, 150,1 )\n",
    "\n",
    "result = np.argmax(model.predict(img))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('./test_data/heart.jpg',cv2.IMREAD_GRAYSCALE)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(cv2.cvtColor(img,cv2.IMREAD_GRAYSCALE))\n",
    "img = cv2.resize(img, (150, 150))\n",
    "img = img/255.0\n",
    "img = np.asarray(img).reshape(-1,150, 150,1 )\n",
    "\n",
    "result = np.argmax(model.predict(img))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('./test_data/long.jpg',cv2.IMREAD_GRAYSCALE)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(cv2.cvtColor(img,cv2.IMREAD_GRAYSCALE))\n",
    "img = cv2.resize(img, (150, 150))\n",
    "img = img/255.0\n",
    "img = np.asarray(img).reshape(-1,150, 150,1 )\n",
    "\n",
    "result = np.argmax(model.predict(img))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('./test_data/round.jpg',cv2.IMREAD_GRAYSCALE)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(cv2.cvtColor(img,cv2.IMREAD_GRAYSCALE))\n",
    "img = cv2.resize(img, (150, 150))\n",
    "img = img/255.0\n",
    "img = np.asarray(img).reshape(-1,150, 150,1 )\n",
    "\n",
    "result = np.argmax(model.predict(img))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('./test_data/trangle.jpg',cv2.IMREAD_GRAYSCALE)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(cv2.cvtColor(img,cv2.IMREAD_GRAYSCALE))\n",
    "img = cv2.resize(img, (150, 150))\n",
    "img = img/255.0\n",
    "img = np.asarray(img).reshape(-1,150, 150,1 )\n",
    "\n",
    "result = np.argmax(model.predict(img))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_save/face_classfier_V0.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_39_cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
